[ { "title": "视频Jitter Buffer原理", "url": "/posts/Video-Jitter-Buffer/", "categories": "public", "tags": "RTC, public", "date": "2023-04-07 16:02:00 +0800", "snippet": "本文以WebRTC中的FrameBuffer为例，介绍了基于卡尔曼滤波的Jitter Buffer的原理。前言由于网络带宽以及视频码率都不是恒定的，当视频数据在网络传输时，各帧数据实际上并不是按照帧率匀速到达的，即存在抖动（Jitter）。为了保证在线播放视频时帧率稳定，不出现快慢放，客户端往往会在本地缓存一段数据，再按照帧率读取渲染。这个机制我们可以称之为Jitter Buffer。显然的，允许缓存的数据越多，视频卡顿的概率越低。在点播场景下，我们可以简单地设置一个较大的Buffer来换取播放的流畅性。但对于实时音视频通话等时延敏感应用来说，增大缓存意味着增加时延，会极大地降低用户体验。所以如何决策出合适的Buffer大小就成为了一个需要优化的内容。注意本文中衡量Jitter Buffer大小的单位都使用时间，而非字节数，这样便于表达。Jitter Buffer多大最合适笔者在思考这个问题时，第一个想法是，直接统计传输耗时最长的那一帧，计算让它不卡顿需要预留多长时间，就可以作为Jitter Buffer的大小了。但是很显然WebRTC并没有这么做，不然也就不会有本文了。原因可能是样本的噪声以及复杂多变的网络环境让这种统计结果变得不够可靠与实时，导致效果欠佳。那么为了设计更好的方案，首先整理一下Jitter的原理。这里将Jitter的产生原因归一下类： 视频的关键帧机制以及码率本身的不稳定导致每帧的大小不同，所以各帧需要不同的传输时间 网络本身的变化，链路切换、拥塞、噪声等 网络存在丢包，重传数据会增加rtt级别的时延WebRTC中的Jitter Buffer基本仅考虑前两点。虽然在代码中能看到有尝试过针对丢包进行优化，但是实现方案很原始，默认并没有开启。笔者认为原因有以下几点： WebRTC的带宽估计算法是基于时延变化运作的，相比基于丢包的算法敏感度更高，产生拥塞丢包的可能性较低 WebRTC有实现FEC（前向纠错），可以不依靠重传解决部分丢包场景 即便Jitter Buffer成功抗住了重传导致的抖动，其代价往往也是百毫秒以上的时延增加。对于RTC应用来说，是偶尔看到视频卡一下，还是全程顶着高时延通话，其实不太好说哪个更体验好所以本文接下来也不考虑重传的影响。接下来，给出理论上各帧视频数据的传输时间：\\[T = T_{tunnel} + \\frac{Size}{rate_{recv}} \\tag{1}\\]式中的$T_{tunnel}$指网络固有延迟，注意当对端发送码率大于网络实际承载能力时，随着阻塞在网络上的数据逐渐累积，该值亦会逐渐增涨。而$rate_{recv}$（接收速率）我们可以认为是对端发送码率和网络实际带宽的较小值。因为越大的帧传输越慢，所以理论上只要我们给最大的帧留够buffer，让它在传输完成时刚好赶上需要播放它的时间，就可以保证视频能够稳定匀速地播放了。因此，需要的buffer大小，或者说需要主动预留的延迟时间，可以大致按照下式计算：\\[Buffer = T_{max} - T_{avg} = \\frac{Size_{max}-Size_{avg}}{rate_{recv}} \\tag{2}\\]式中的帧大小我们可以在接收端通过统计直接得到，而$rate_{recv}$则是一个需要跟踪的未知参数，接下来介绍其计算方法。接收速率的计算因为帧数据的实际发送不是连续的，而是按照采集帧率有一定间隔地发送。所以直接将帧大小除以两帧之间的时间间隔是不行的。这里我们只能通过计算不同帧的差值来计算接收码率：\\[rate_{recv} = \\frac{Size_{delta}}{T_{delta}} \\tag{3}\\]式中的$T_{delta}$如下图中所示。Tdelta的获取由于该值一般很小，它的测量精度和噪声导致我们没办法直接使用它计算。在工程应用中，我们一般使用各类滤波器（Filter）来处理这种情况。即对样本做加权处理，以得到比较稳定且准确的预测结果。最简单的例子就是求平均值。而当系统的状态会随时间变化时，我们也可以通过逐渐降低靠前的样本的权重来跟踪。但是简单的滤波器严重依赖权值的定义，跟踪的响应速度和准确性都十分有限，不能保证在所有网络环境下都好好工作。WebRTC在这里采用了有名的卡尔曼滤波[1]，其运算量低的同时效果也很优秀，并且能够同时输出参数的概率分布。不了解卡尔曼滤波但感兴趣的读者可以阅读此文章，讲得比较通俗易懂。本节接下来的内容需要一定卡尔曼滤波相关知识。不感兴趣的读者可以跳到下一节。WebRTC对卡尔曼滤波的实现，集中在jitter_estimator.cc文件中，主要分为两个函数。一个用于更新模型，计算接收速率；一个用于更新样本与预测值的偏差，在最后计算结果时会使用。本文并不打算对着卡尔曼滤波公式一个一个讲解，这里先贴一下核心代码，后续补充解释：void VCMJitterEstimator::KalmanEstimateChannel(int64_t frameDelayMS, int32_t deltaFSBytes) { double Mh[2]; double hMh_sigma; double kalmanGain[2]; double measureRes; double t00, t01; // Prediction // M = M + Q _thetaCov[0][0] += _Qcov[0][0]; _thetaCov[0][1] += _Qcov[0][1]; _thetaCov[1][0] += _Qcov[1][0]; _thetaCov[1][1] += _Qcov[1][1]; // Kalman gain // K = M*h&#39;/(sigma2n + h*M*h&#39;) = M*h&#39;/(1 + h*M*h&#39;) // h = [dFS 1] // Mh = M*h&#39; // hMh_sigma = h*M*h&#39; + R Mh[0] = _thetaCov[0][0] * deltaFSBytes + _thetaCov[0][1]; Mh[1] = _thetaCov[1][0] * deltaFSBytes + _thetaCov[1][1]; // sigma weights measurements with a small deltaFS as noisy and // measurements with large deltaFS as good double sigma = (300.0 * exp(-fabs(static_cast&amp;lt;double&amp;gt;(deltaFSBytes)) / (1e0 * _maxFrameSize)) + 1) * sqrt(_varNoise); hMh_sigma = deltaFSBytes * Mh[0] + Mh[1] + sigma; kalmanGain[0] = Mh[0] / hMh_sigma; kalmanGain[1] = Mh[1] / hMh_sigma; // Correction // theta = theta + K*(dT - h*theta) measureRes = frameDelayMS - (deltaFSBytes * _theta[0] + _theta[1]); _theta[0] += kalmanGain[0] * measureRes; _theta[1] += kalmanGain[1] * measureRes; // M = (I - K*h)*M t00 = _thetaCov[0][0]; t01 = _thetaCov[0][1]; _thetaCov[0][0] = (1 - kalmanGain[0] * deltaFSBytes) * t00 - kalmanGain[0] * _thetaCov[1][0]; _thetaCov[0][1] = (1 - kalmanGain[0] * deltaFSBytes) * t01 - kalmanGain[0] * _thetaCov[1][1]; _thetaCov[1][0] = _thetaCov[1][0] * (1 - kalmanGain[1]) - kalmanGain[1] * deltaFSBytes * t00; _thetaCov[1][1] = _thetaCov[1][1] * (1 - kalmanGain[1]) - kalmanGain[1] * deltaFSBytes * t01;}其实注释已经解释了大部分内容，这段代码将测量样本带入卡尔曼滤波的迭代公式来更新模型。其状态外推方程（State Extrapolation）和协方差外推方程（Covariance Extrapolation）都设定为了不变（乘以单位矩阵）。系统状态参数是_theta[0]和_theta[1]，其观察方程即代码deltaFSBytes * _theta[0] + _theta[1]。可以按下式理解：\\[T_{delta} = Size_{delta} * \\frac{1}{Rate_{recv}} + T_{offset} \\tag{4}\\]_theta[0]就是$Rate_{recv}$的倒数，上文解释过，这是我们的目标值。而_theta[1]（$T_{offset}$），就是一个和帧大小无关的偏移量。当网络处于逐渐拥塞的状态时，其值为正，当网络从拥塞中恢复时，其值为负。另外，代码中的sigma变量即测量不确定度（Measurement Uncertainty），就像注释里说的，因为deltaFSBytes越大，时间差越大，样本精度就越高，所以根据其大小给予不同权重。目标Buffer的计算每收到一帧，我们都得到一个测量样本（frameDelayMS和deltaFSBytes），通过不断迭代更新模型，即可获取到接受速率。似乎接下来就可以直接根据公式(2)来计算Jitter Buffer的大小了。但是这还不够，因为这是个有噪声的系统，而前面计算出的仅仅是期望值。如果直接使用这个值作为buffer，即便预测值是准确的，系统在接收到最大帧时仍然有50%的几率卡顿。这里我们还应该考虑噪声的分布。这也是WebRTC采用卡尔曼滤波的原因之一。这部分内容较为简单，即高中学习的正态分布置信区间的应用。每次拿到样本时，除了更新卡尔曼滤波模型，还要统计模型预测结果和实际测量值的偏差（即统计样本和公式(4)的差值），计算其方差$σ^2$。然后就可以得到在期望值上需要增加的偏移量了：\\[T_{offset} = 2.33 * σ - 30ms \\tag{5}\\]式中的常量都是WebRTC定义的经验值。2.33是正态分布表（z-table）中99.01%的那一项，整个式子表示算法可以接受有1%的概率出现超过30毫秒的卡顿。最后，再加上公式（2），我们就得到了Jitter Buffer的最终输出：\\[Buffer =\\frac{Size_{max}-Size_{avg}}{rate_{recv}} + T_{offset} \\tag{6}\\]遗留问题及总结本文介绍了Jitter Buffer的核心逻辑。但是遗留了两个问题笔者还没有想清楚。一个是卡尔曼滤波的观察方程建模包含了一个偏移量（_theta[1]），但计算Jitter Buffer时却并没有使用它。虽然感觉可以接受，毕竟只有网络拥塞时这个值才有意义，但还是缺乏一个完整的解释。还有一个问题是卡尔曼滤波本身已经统计了系统状态的概率分布了（即协方差矩阵_thetaCov），直接使用它应该也可以得到类似式（5）中的$T_{offset}$。但是WebRTC额外进行了统计，原因是什么呢。如果读者有答案，欢迎讨论。另外，在实际应用Jitter Buffer时，还涉及到等待队列控制，各种时间戳的转换，音画同步，异常值保护等等内容。因为较为琐碎，这里就不详谈了，感兴趣的读者可以直接阅读WebRTC的源码timing.cc和frame_buffer2.cc。参考文献[1] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. J. Basic Eng. Mar 1960, 82(1): 35-45 (11 pages)" }, { "title": "GCC带宽探测原理", "url": "/posts/GCC-Bandwidth-Detection/", "categories": "public", "tags": "RTC, public", "date": "2022-01-18 20:16:00 +0800", "snippet": "前言拥塞控制是RTC系统中非常重要的一个功能模块，直接关系到通信质量。而在谷歌开源的WebRTC中，就有着名为GCC（Google Congestion Control）的优秀实现供我们参考学习。但是由于其代码经历大量迭代，兼容和实验性质的代码较多，不利于阅读。本文将结合流程图分析GCC的带宽探测过程。使用的WebRTC版本为72，分析内容不包含默认不开启的实验代码和废弃代码。阅读本文需要对RTP协议有一定了解。TransportSequenceNumber 与 TransportCC根据协商结果的不同，WebRTC会采用不同的方式评估带宽。当上层协商启用RTP拓展AbsSendTime以及RTCP类型REMB时，带宽的计算工作主要集中在接收端；而当启用RTP拓展TransportSequenceNumber以及RTCP类型TransportCC时，计算工作则几乎全在发送端。由于后者相对更新，并且按照WebRTC开发人员的说法也相对更好一些[1]，所以本文仅分析发送端计算带宽的情况。如果读者对TransportSequenceNumber和TransportCC不甚了解，可以将前者理解为附加在每个媒体包后的一个递增的编号，而后者则是接收端每隔一段时间发送的汇总反馈。该反馈中包含接收到的编号以及对应的时间信息。GCC上层代码结构首先我们看一下GCC的上层代码结构：GCC上层代码结构 SendSideCongestionController: 发送端拥塞控制的总控制类。该类在72版本中仍处于重构阶段，有两个同名实现，所属不同的命名空间，并且在将来会被移除。不过它的移除并没有影响下层逻辑，所以对本文影响不大。 NetworkControllerInterface: 该接口接收各类网络事件，并返回带宽探测的结果。 GoogCcNetworkController: 基于GCC的拥塞控制模块，与它平级可选的还有BbrNetworkController和PccNetworkController，只不过代码中没有启用。另外，新的WebRTC已经移除了BBR，原因是perform badly for WebRTC purposes。 CongestionControlHandler: 负责将GCC的评估结果发送到Pacer和BitrateAllocator。Pacer将数据包以一定速率均匀地发送至网络，防止引发拥塞和丢包。同时Pacer还具备发送padding包和历史包的能力。历史包可以在丢包时重传，也可以当作padding包单纯用来辅助带宽探测。BitrateAllocator负责将总带宽根据事先配置的优先级和上下限分配给各个数据流。 ProbeController: 控制探测数据包的发送。在特定情况下会以指数递增形式发送探测包，从而快速探测出当前带宽。 ProbeBitrateEstimator: 评估探测包的接收码率，作为后续评估的参考。 AlrDetector: Alr全称Application limited region。该类评估当前应用产生的数据量是否受限（即是否未达到分配给它的码率）。若存在受限，则会通过padding等手段填充数据，防止探测出来的带宽过低。Alr一般是由于画面内容过于静止，视频编码器的输出码率无法达到目标码率导致的。 AcknowledgedBitrateEstimator: 计算对端实际接收到的码率，作为后续评估的参考。 DelayBasedBwe: 根据时延变化的趋势评估带宽，作为后续评估的参考。 SendSideBandwidthEstimation: 该类汇总delay based bandwidth和loss rate等信息，得出GCC评估带宽的最终结果。顺便一提，其实GCC下还有一个LossBasedBwe，即基于丢包统计的带宽估计，和DelayBasedBwe相呼应。不过它默认是不开的，所以本文就不介绍了。下图表达了一个典型的，由TransportCC触发网络模型更新的例子，读者可以大致感受一下它们是怎么配合工作的。各个子模块的细节将在接下来几节中介绍。GCC网络模型更新流程的例子带宽的主动探测（ProbeController与ProbeBitrateEstimator）RTC中的探测（probe）指的是使用大量探测包，主动探测当前带宽上限的行为。一般有以下几个原因会触发probe询问事件： 会话刚刚建立 当前使用的网络连接发生切换 用户重新配置了可用的最大带宽 应用从网络拥塞中恢复（即网络时延由高位下降至一个稳定值时） 探测到的带宽发生变化 GCC本身的定期处理（默认25ms一次）发生这些事件后，GCC会向ProbeController询问是否需要主动探测带宽，如果需要，那具体要使用多大的码率。接着GCC再将结果发送给Pacer。Pacer则根据这个码率发送探测包。每次探测一般持续数十毫秒，并且pacer会优先使用有意义的媒体数据包，如果不够，再用纯0的padding包补充。探测包的分配ProbeController的内部结构是一个简单的状态机：ProbeController状态机在不同的状态下，ProbeController会对不同的询问事件做不同的处理，下面仅列举几个比较重要的规则： 当处于kInit状态时，进行两次探测。第一次的探测码率是用户配置的初始码率的3倍，第二次是6倍，同时状态迁移至kWaitingForProbingResult。 当处于kWaitingForProbingResult状态，并且触发询问事件的原因是探测到的带宽发生了变化时，若这个新带宽大于之前发送的探测包码率的0.7倍，则以新带宽的2倍进行下一次探测；若低于0.7倍，则不做处理。 当处于kWaitingForProbingResult状态，并且距离上一次探测到的带宽发生变化已经过去了1秒以上时，状态迁移至kProbingComplete。 当处于kProbingComplete状态，并且触发询问事件的原因是应用从网络拥塞中恢复时，查询最近是否有进入alr，如果有，则以当前带宽的2倍进行一次探测。通过探测包的反馈估计带宽Pacer得到目标探测码率后，开始发送探测包。接收端不需要识别是否是探测包，统一根据TransportCC协议返回feedback即可。发送端收到feedback后，根据id判断它对应的是探测包还是普通媒体包。如果是探测包，则传给ProbeBitrateEstimator，用于计算主动探测的带宽。ProbeBitrateEstimator首先用该组探测包的总数据量除以首包和尾包的时间间隔得到码率，再按照下图流程进行调整。图中接收码率低于发送码率的0.9倍时，算法认为发送码率已经达到网络通道的瓶颈，为了防止发生网络拥塞，所以额外降低了码率。主动探测带宽的计算过程可以看到整个主动探测过程使用了大量的经验值，实现并不美观，令人担心其普适性。幸运的是主动探测的主要作用仅仅是在对话开启或网络发生状况时快速上探带宽，防止画面长时间模糊。上探完成后，GCC有更精细的算法去进一步调整评估结果。对端接收码率评估（AcknowledgedBitrateEstimator）该类利用TransportCC的feedback计算对端实际接收到的码率，乍看之下和上节的ProbeBitrateEstimator功能类似。但实际上，由于probe过程是分组进行的，而媒体数据的发送却是连续不断的，所以后者的接收码率计算方式可以设计得更鲁棒一些。实际上，该类是通过贝叶斯估计的方式不断更新码率的，这里直接上源码，还是挺好读的： // 以150ms为周期，计算该时间段内的码率，作为一个码率样本 bitrate_sample // ..... // 利用此样本更新码率bitrate_estimate_以及其他参数 float sample_uncertainty = 10.0f * std::abs(bitrate_estimate_ - bitrate_sample) / bitrate_estimate_; float sample_var = sample_uncertainty * sample_uncertainty; // Update a bayesian estimate of the rate, weighting it lower if the sample // uncertainty is large. // The bitrate estimate uncertainty is increased with each update to model // that the bitrate changes over time. float pred_bitrate_estimate_var = bitrate_estimate_var_ + 5.f; bitrate_estimate_ = (sample_var * bitrate_estimate_ + pred_bitrate_estimate_var * bitrate_sample) / (sample_var + pred_bitrate_estimate_var); bitrate_estimate_var_ = sample_var * pred_bitrate_estimate_var / (sample_var + pred_bitrate_estimate_var);基于时延的带宽估计（DelayBasedBwe）该类是GCC最重要的模块，包含两个主要功能。一是根据TransportCC反馈的时间信息来探测当前网络状态；二是根据得到的网络状态调整预测带宽。前者由成员TrendlineEstimator实现，后者由成员AimdRateControl实现：DelayBasedBwe的构成网络拥塞状态的探测（TrendlineEstimator）TCP传输一般是根据丢包来判断网络是否发生拥塞的。但是发生丢包时，传输时延往往已经由于网络节点的buffer而增加了。所以对于时延敏感的RTC应用来说，需要更早的探测到拥塞。最直接的办法就是根据时延的变化来探测。在TransportCC中存储着一组媒体包的对端接收时间（arrive_time, AT）。利用它和本地的发送历史（send time，ST），可以计算出每个媒体包的网络传输耗时的变化：\\[delta = (AT_{2} - ST_{2}) - (AT_{1} - ST_{1})\\]当这个delta变大时，我们就可以认为网络发生了拥塞。在TrendlineEstimator出现之前，GCC是使用卡尔曼滤波来评估网络状态的。现在谷歌换成了更简单的Trendline，其原理是对delta样本进行一阶线性拟合，根据结果的斜率来判断网络状态。斜率大于0时是Overusing，小于0时是Underusing，约等于0时是Normal。这里要注意Underusing这个状态，他比较容易造成误解。一般来说，只有先发生拥塞，时延才有下降的可能性。也就是说，Underusing表达的是网络正在从拥塞中恢复，而不是网络带宽没有被充分利用。根据网络状态调整带宽（AimdRateControl）熟悉TCP的读者应该对AIMD很熟悉。它的意思是加性增（Additive Increase），乘性减（Multiplicative Decrease）。其目的是在网络正常时缓慢增加传输速率，而在拥塞时快速减少，从而在避免拥塞的同时尽可能利用带宽。AimdRateControl先根据Trendline的探测结果迁移内部状态机的状态，再结合该状态与rtt、acknowledged bitrate等信息决策带宽：AimdRateControl内部状态机AimdRateControl处理流程这里对流程图做几点说明： 为了保证输出带宽曲线的平滑，图中使用的rtt默认是ReceivedReport（一种对端反馈的RTCP包，包含的信息能够帮助本地计算出rtt等信息[2]）中的rtt的平均值。如果经过配置，也可能使用TransportCC来计算平均rtt。 1200Bytes / respond_time * duration 中的1200Bytes表示一个网络包的平均大小，respond_time暂时可以看做rtt，duration指的是两次处理的间隔。也就是说，这个计算追求的效果是每个rtt多发一个包。这和TCP的增窗算法很像。 至于为什么rtt要额外加100ms，按照代码注释的说明，应该是给拥塞探测的延迟预留的。考虑到TransportCC的传输间隔和时间，以及线性拟合本身需要一定的数据量才能体现出时延的变化趋势。从拥塞实际发生到trendline探测到它之间一般有几十到几百毫秒的延迟。所以这里保守起见预留了100ms延迟，进一步降低网络拥塞对通信质量的影响。当然，代价就是某些情况下带宽上探得更慢了。 图中省略了许多输出上下限Clip的操作，这些操作可以防止带宽波动过大以及无限制的上探等问题，不过全都贴上来就显得太琐碎了。带宽的最终决策（SendSideBandwidthEstimation）DelayBasedBwe得出结果后，GCC会将其和rtt、lossrate等信息一起传给SendSideBandwidthEstimation。并且周期性的调用后者的UpdateEstimate()接口更新评估带宽。这个带宽就是GCC的最终输出了，它会被传给bitrate_controller用作带宽分配。下图是UpdateEstimate()的大致流程。网络状态良好时除了乘以1.08还额外加了1000的目的是防止在带宽过低时上探速度过慢。另外大于15秒超时的时候码率下降至0.8倍实际是实验性质的代码，默认并没有开启。UpdateEstimate总结本文仅概述了GCC最基本的发送端带宽探测算法，省略了大量默认不启用的模块，包括但不限于LossBasedBandwidthEstimation，CongestionWindowPushbackController，LinkCapacityTracker。并且考虑到WebRTC版本更新频繁，代码差异巨大。建议读者仅借助本文体会GCC大体的工作流程，钻研细节的工作还是留在读者实际使用的版本上吧。参考文献[1] https://groups.google.com/g/discuss-webrtc/c/ZyKcu3E9XgA/m/hF0saddeLgAJ [2] https://www.freesoft.org/CIE/RFC/1889/18.htm " }, { "title": "视频码率控制原理", "url": "/posts/Video-Rate-Control/", "categories": "public", "tags": "RTC, video encoding, public", "date": "2021-11-17 20:16:00 +0800", "snippet": "本文简要总结了笔者对视频编码技术中码率控制算法的理解。例子的范围限制在从H.264至今的编码标准，更早的编码标准笔者并没有了解过。名词解释 视频编码（video encoding）：将视频的原始像素数据压缩为符合某些标准（比如H.264）的二进制数据的过程。 码率（bitrate）：单位时间内的数据量，一般用bps（bits per second）作为单位。 MB：宏块（macro block），实际上在不同的编码标准中它有不同的叫法，但本文使用H.264/AVC中的MB。它是视频编码的基本单元，常见的大小有16x16、64x64等。编码时每一帧画面都会被按照固定大小分割成大量MB，再一个个处理。 QP：量化参数（quantization parameter）。视频压缩一般为有损压缩，编码时需要为每一帧以及每一个MB选择QP，用以控制画面质量与码率。QP越大，则损失的信息越多，画面质量越差，但压缩率也越高。 Q step：量化步长（quantization step）。上面提到的QP实际仅是个序号，在熵编码时使用。真正用来对数据做量化的其实是量化步长。它是一组浮点数，与QP一一对应。如果你不清楚什么是量化，可以通俗地理解为一种调整数据精度的操作。比如测量物体重量，精度要求高时用克，精度要求低时用吨。码控的类型视频编码的码率控制（Rate-Control），主要指编码过程中，通过调整QP来控制视频码率的行为。在不同的使用场景下，用户对视频的码率大小、稳定性会有不同的要求，因此码率控制的类型也多种多样。比如观看离线视频文件时，往往不需要此视频具备恒定的码率。因为整个文件都在本地，只要磁盘IO和设备解码能力都合格，基本就不会发生卡顿问题（这里不讨论硬件解码的buffer限制以及内网带宽限制）。所以压制这类视频时会倾向于在相同的文件大小下保证画面质量而非码率平稳性。画面纹理比较复杂或运动剧烈的场景，码率给高一些，以保证画面质量；而画面简单的场景，码率就给低一些，节省硬盘空间。这种码控策略我们统称为可变码率（VBR）。但在观看在线视频时，由于用户带宽有限，客户端缓存的数据量也有限，所以需要对码率做一些限制。否则一些瞬时码率过高的片段可能会引起卡顿。此外还有一点，由于CDN是按流量计费的，视频网站如果使用VBR编码视频会使带宽成本变得不可控。所以压制这类视频时，会倾向于选择恒定码率（CBR）。不同场景下的码控需求（相同文件大小的前提下）本文接下来的讨论内容主要集中在CBR，但其中的一些改良算法其实也可以用在VBR。码控的根基：RQ模型首先看一下码率控制的大体流程：码控基本流程可以看到整个流程都是围绕着RQ模型进行。这里的RQ模型指的是码率（Rate）与量化参数（QP）的推导关系，可以说是码控算法的根基。它经历过多次改进，精度也在不断提高，这里举几个例子： 一阶模型：\\[R = α \\cdot \\frac{Comp}{Q} \\tag{1}\\]最简单的模型，在x264、OpenH264等编码器中被广泛使用。式中Comp表示图像复杂度，一般可以使用MAD（Mean Absolute Difference）来计算。α是一个系数，需要我们在编码过程中不断根据实际输出码率来更新。 二阶模型：\\[R = α \\cdot \\frac{Comp}{Q} + β \\cdot \\frac{Comp}{Q^2} \\tag{2}\\]一阶模型的进阶版，JM中有使用。 R-λ模型：\\[\\begin{cases} R = α \\cdot λ^β \\\\ Q = f(λ) \\end{cases}\\tag{3}\\]在HM（HEVC Test Model）中被引入的模型[1]，显著提升了码控精度。式中的λ是率失真优化中使用的拉格朗日乘子，放在下节作为拓展内容介绍。而f(λ)则是通过实验拟合得到的。比如HM中就是：\\[QP = 4.2005lnλ+13.7112 \\tag{4}\\]MAD值的获取与模型的更新上面提到RQ模型中使用的图像复杂度可以使用MAD来计算。但是当前帧的MAD值只能在编码后才能得到，因为MAD的计算需要用到编码产生的数据，这与QP需要在编码前确定这一点相矛盾。另外，在模型确定后，我们还需要确定模型的更新方式及速率。通过提高模型系数的更新速率，可以使编码器的输出码率更加平稳；但相对的，画面质量可能由于QP变化幅度过大而下降。针对这些问题，不同的编码器选择了不同的方案： 在JM中，使用了一个一阶线性模型根据过去帧的MAD来预测当前帧的对应值。而RQ模型的系数α则直接通过当前帧的编码结果来反向计算得到。 在OpenH264和x264中，则放弃了MAD的计算，转而合并了$α \\cdot Comp$，把它作为一个整体，用编码结果来更新。 对于R-λ模型，其提出者也已经设计了对应的更新公式和经验值供我们参考[1]。模型更新这部分内容，更多是工程实践上的考量，需要根据经验和测试情况不断调整。拓展：λ是什么R-λ模型中的λ是率失真优化（Rate-distortion optimization，RDO）中的一个系数。这里的率失真优化，指的是编码过程中的一个优化步骤。要详细解释它，我们需要先了解率失真理论。它起源于香农的研究[2]，大致可以概括为：在给定的信源分布以及可接受的失真度D下，求信息数据量R的理论最小值。显然，可接受的D越大，其对应的R也就越小。这个R-D的关系边界，我们称为率失真曲线，即下图中的红线[3]。旁边的绿线则是编码器实际工作区域的边界，它和理论值存在一定差距。R-D曲线视频编码标准提供了大量的工具集，或者说编码模式，它们就相当于上图的绿点，使用它们会产生不同大小的失真和压缩率。在它们之中选择一个恰当的来使用是一件比较复杂的事情。注意到这个问题本质是一个数学最优化问题，所以可以利用拉格朗日乘数法的离散形式来解决它。简单地说，引入一个拉格朗日乘子λ，构建代价函数，再去求取代价函数的最小值即可：\\[\\begin{cases} min\\{J\\} \\\\ J(λ)=D+λ \\cdot R \\end{cases}\\tag{5}\\]通俗地理解，λ就相当于失真与码率的权值。给定了λ，也就给定了一个判定模式好坏的标准。如下图所示，求代价函数最小值，相当于求斜率为λ的直线与R-D曲线的切点。在编码过程中，不论是模式选择，还是运动矢量的比较，都会利用到这个方法。拉格朗日乘子与R-D曲线斜率R-λ模型的提出者首先根据实验数据提出R-D曲线可以近似拟合为一条双曲线，再根据λ是R-D曲线斜率这一点，求得了R-λ模型：\\[\\begin{cases} D(R)=CR^{-K} \\\\ λ=-\\frac{∂D}{∂R}=CKR^{-K-1}=α&#39; \\cdot R^{β&#39;}\\end{cases}\\tag{6}\\]码控优化算法上文介绍的内容，属于码控核心算法，其目的仅为控制输出码率，使其尽可能地接近目标。但在实际应用中，我们并不需要这么严格地执行。在输出码率基本满足要求的前提下，我们依然有一定的调整空间来改善画面质量。下文将介绍几种这方面的算法。Adaptive-Quantization由于人眼对不同类型的画面的细节感知程度是不同的，所以给不同画面内容给予相同的权重来分配码率实际上是比较浪费的。比如高速运动的物体就可以相对静态物体模糊一点。在这方面做文章的算法一般归类为基于感知的码控算法。而其中一类比较简单且使用广泛的算法，叫做Adaptive Quantization（AQ）。简单来说，它利用图像的方差或其他类似特征来衡量当前宏块的复杂度，对于内容较复杂的宏块，算法认为可以适当舍弃其细节，所以增大其QP；而简单的宏块则反之。经过这种调整，视频的主观质量能得到大幅提升。如今很多编码器都实现了AQ，比如x264的实现如下所示。计算得到的qp_adj即QP偏移量，会叠加到R-Q模型计算出来的QP上：// x264的AQ核心实现。ac_energy_mb计算宏块像素值方差。strength代表AQ强度，由用户配置。uint32_t energy = ac_energy_mb( h, mb_x, mb_y, frame );qp_adj = strength * (x264_log2( X264_MAX(energy, 1) ) - (14.427f + 2*(BIT_DEPTH-8)));利用人眼感知能力的码控算法还有很多，比如基于最小可觉察误差(JND, Just Noticeable Distortion)的，或者基于机器学习的，这里不再展开。值得一提的是，由于是针对主观视觉体验的优化算法，通过PSNR等客观手段测试往往无法体现这些算法的优势。MB-TreeMB-Tree是由x264的开发者提出的算法，它的基本思想是：由于视频数据在编码时存在依赖关系，被参考的数据的失真程度直接影响到后续的预测精度，所以根据依赖关系给予不同画面不同码率权重可以从整体上提升画面质量。MB-Tree的实现依赖于x264内部的lookahead结构。所谓的lookahead，是一种预编码模块。它会使用经过下采样的低分辨率图像预先对视频进行一轮编码（这种预编码只进行到SATD的计算），并缓存大量有用的信息供后续正式编码使用。我们在使用x264编码视频时，经常会看到-lookahead参数，它表示预编码的帧数，增加这个值利于提升编码质量，但是会增加编码时延。有了lookahead缓存下来的信息，我们就可以从最后一帧开始，逐帧回溯每个MB的依赖性，或者说重要性，再根据这个重要性计算QP的偏移量。MB-Tree的原理非常有特色，如果读者对细节感兴趣，推荐阅读x264开发者的文章[4]。总结本文仅概述了一些码控算法的基本思想和原理，想要深入理解这些算法，还是需要阅读论文和源码。笔者看过的代码中，JM和OpenH264的码控算法属于比较容易阅读的（起码比x264容易懂），推荐感兴趣的读者从这两个开始。参考文献[1] JCTVC-K0103[2] C.E. Shannon. Coding theorems for a discrete source with a fidelity criterion. In IRENational Conventwn Record, Part4, pp. 142-163, 1959.[3] A. Ortega; K. Ramchandran. Rate-distortion methods for image and video compression. IEEE Signal Processing Magazine, Volume: 15, Issue: 6, Nov 1998.[4] Garrett-Glaser J. A novel macroblock-tree algorithm for high-performance optimization of dependent video coding in H.264/AVC[J]. Tech. Rep., 2009." }, { "title": "视频隐形水印技术分类", "url": "/posts/Video-Stego/", "categories": "public", "tags": "video processing, public", "date": "2021-09-30 09:30:00 +0800", "snippet": "说明：本文是笔者之前给公司公众号写的两篇隐形水印科普文章的整合与扩充，介绍了能够在视频数据中嵌入隐形数据的技术。前言起初笔者只是被论坛里一个讨论隐写术的帖子引起了兴趣，期望找到一种隐藏的信息经过转码甚至屏摄后依然不会丢失的视频隐写技术，即一种可靠的隐形数字水印技术。遗憾的是一番资料查下来，发现现有的技术和笔者的期望依然存在一定距离。但查都查了，这里还是分类总结一下。封装级水印首先是实现最简单的封装级水印。视频封装即视频容器层的数据，通俗地理解就是文件格式。这里贴张图科普一下视频文件的基本构成。视频文件的基本结构因为这一层数据是和外界直接交互的，所以只要熟悉格式标准就能通过改写二进制数据来嵌入数据。曾经流行的图种就是一种最简单的图像封装层的隐写。也许有些人不知道图种是什么。简单地说，如果你从网络上下载了一张jpeg图片，用解压软件对它进行解压，能得到其他文件，比如某某种子，那么这种隐藏了数据的图片就叫图种。它过去在网络论坛上被广泛使用，人们用它来传播各种无版权资源。制作图种很简单，只需要一条命令，把jpeg文件和zip文件进行二进制拼接即可。cat a.jpg b.zip &amp;gt; output.jpg图种的生成与解压图种可以被正常显示为什么这种文件能正常工作呢。因为jpeg格式的文件，是以一串固定数据（0xFFD9）结尾的[1]；而zip格式的文件，也有其固定的起始码（0x04034B50）[2]。现在大部分图片浏览器和解压软件都有对数据首尾位置不正常的情况做兼容，所以它们会直接忽略首尾标记范围以外的数据。那么，图种毕竟只是图片，如果是格式更复杂的视频，是否也存在利用封装结构来隐藏数据的方法呢？那可太多了。事实上，大部分视频格式都有存储附属信息的结构，播放器在解析文件时基本都会忽略这些信息，因为它们不知道怎么处理这些东西。所以我们只要把数据藏在那里就行了。下面简单举几个例子。flv文件flv文件由一个header和大量的tag组成，这些tag分为三类：audio、video和script data。其中script data的格式标准支持插入各类自定义数据[3]。我们可以对照官方文档来实现代码修改这个tag的数据。嫌麻烦的话，一些开源软件也支持在一定程度上的修改它，比如flvmeta，虽然这个工具只支持插入字符串:flvmeta -U -a keyString:valueString movie.flv使用flvmeta修改flv文件，加入自定义数据实际存储在文件中的数据mp4文件mp4文件由大量的，可嵌套的box组成。因为box的类型非常多，大部分播放器只会选择它需要的box去解析。因此我们可以自定义一些box类型去嵌入数据。但是由于mp4比flv格式复杂不少，还是推荐直接使用辅助工具。这里用比较常见的mp4box来举个例子：# 在mp4中新建一个自定义boxMP4Box -set-meta aaaa sample.mp4# 在刚才新建的box中嵌入数据，id自己定，需要记住MP4Box -add-item wow.jpg:id=111:name=hiddenData:type=bbbb sample.mp4# 导出嵌入的数据，111就是上面嵌入的idMP4Box -dump-item 111:path=dump.jpg sample.mp4顺便一提，笔者使用的1.0.1版本的mp4box，导出meta会有IO错误。起初笔者一直以为是自己使用方式有问题，但下载代码debug后发现其实是mp4box自己的bug。它先close了文件再flush！查了一下master分支这个问题已经修复了，估计下个版本就好了。读者使用这个工具时请注意版本号。H.264码流在flv、mp4、mkv等文件格式上添加数据，意味着视频必须以离线文件的形式存在。如果我们希望在直播或者视频通话中的视频数据上添加水印呢？这时我们可以在更下一层，也就是视频码流层下手。码流即本文第一张图《视频文件的基本结构》中的bitstream，它是编码压缩后的数据，可以被存储在各种封装格式中。视频编码有很多标准，现在使用最广泛的H.264标准中，有一段数据叫做SEI（补充增强信息，Supplemental Enhancement Information），它被用来存储辅助解码与显示的信息，支持添加用户的自定义数据。著名的开源编码器x264就在这段数据里写了自己的版本信息以及编码参数。我们可以参照格式标准生成自己定义的SEI数据，再嵌入视频码流中，从而实现隐形水印。自定义SEI的语法标准[4]x264生成的SEI数据封装结构层的水印是所有隐形水印中运算量最小的，因为它不涉及到视频的编解码。但是其缺点也很明显。视频在被传播的过程中极可能被人重新编码存储，在这个过程中，事先添加在这一层的自定义数据一般都会丢失。因此这类方法仅在一些特殊的场景被使用。空间域（像素域）水印对于大部分人来说，比起封装格式，可能对像素数据更熟悉一些。我们知道一张数字图像一般是由若干个通道的二维数据，也就是很多像素点构成的。单个通道的像素点数据一般是8比特位（HDR会多一点），当其最低位被修改时，一般很难被人眼观察到，所以我们可以把隐藏数据写在这里。这种方法我们称为LSB（Least Significant Bit）方法。下图的十个方块，蓝色通道的像素值依次由246递增至255，相邻的两个方块相当于修改了LSB数据。修改LSB数据较难被肉眼分辨至于具体实现，这里介绍最粗暴的一种。先把水印数据转化成只有0和1的二值图像，然后直接写到目标图像的最低位上，这样就完成了水印的嵌入。下面我们用python简单实现一下：import cv2# 读取图像，将水印缩放至目标图像大小，并二值化ori_img = cv2.imread(&#39;Lenna.jpg&#39;)watermark = cv2.imread(&#39;watermark.jpg&#39;)## 既然是二值图像，使用最近邻方式来拉伸比较合适，可以防止插值造成边缘模糊resized_watermark = cv2.resize(watermark, ori_img.shape[:2], interpolation=cv2.INTER_NEAREST)binary_watermark = resized_watermark &amp;gt;&amp;gt; 7# 嵌入水印并保存结果output_img = ori_img &amp;amp; 0xFE | binary_watermarkcv2.imwrite(&#39;Lenna_with_watermark.png&#39;, output_img)# 提取水印img_with_watermark = cv2.imread(&#39;Lenna_with_watermark.png&#39;)extracted_watermark = ((target_img &amp;amp; 0x01) * 255)cv2.imwrite(&#39;extracted_watermark.jpg&#39;, extracted_watermark)原始图片待添加水印添加完水印的图片提取出的水印细心的读者会发现，代码中其他图像都是使用jpg格式存储的，唯独添加完水印的图片代码中使用了png格式。这是因为最低有效位的数据非常脆弱，极容易被有损压缩算法破坏，导致水印无法正常提取。而png格式是无损压缩格式，不会引入这个干扰。如果把上述代码中的png换成jpg，你会看到提取出的水印变得完全无法辨认。即便我们以增加视觉损失为代价把写入的比特从最低位提高一些，依然不能保证水印的完整性。图片经过有损压缩后提取的水印LSB类型的隐形水印，或者说隐写术，因为其原理通俗易懂，所以使用比较广泛。它经常出现在一些线上解谜中，也有一些现成的工具帮助分析，比如stegsolve。另外，一些公司会用它在内部文档上做水印，用来在泄漏时追究员工责任。变换域水印LSB方法依然存在添加的水印经过有损压缩后容易丢失的问题。为了提高水印的鲁棒性，一些人提出了在变换域上添加水印的方法。所谓变换域，是相对前一节的空间域（像素域）而言的。一般我们管原始的像素数据叫空间域（Spatial Domain）系数，因为每一个像素都代表它所在坐标的空间位置的采样值。当我们希望从另一个角度分析图像，就会对图像进行线性变换，也就有了各种变换域。最常用的变换域就是频率域，它有很多优秀的性质方便我们分析和操作数据。频域变换示意图[5]基于DCT的方法DCT即离散余弦变换，它的2D形式可以把空间域图像转换到频率域，在图像处理领域被广泛应用。关于它的细节，可以阅读此链接：详解离散余弦变换（DCT）。不过其实只要了解个大概，也能读懂本节内容。基于DCT的隐形水印是一类很常见的隐形水印算法。选择DCT的原因有很多。 一是人眼对图像中不同频率的信号敏感程度不同，直接在频率域上操作数据有利于控制主观感知到的失真程度，以保证水印的隐形。 二是不同频率的信号稳定性不同，在频率域加水印有助于控制水印的鲁棒性，保证水印在载体经历各类损伤后依然能够还原出来。 三是理论上这类方法可以直接嵌入一些编码器，从而减少运算量。比如jpeg压缩过程中就使用了DCT。但需要注意的是，上述一二两点其实是有矛盾的，水印数据所属的频率范围越低，鲁棒性越高，但图像的视觉损失也越大，反之亦然。因此大部分实现会选择在中频范围内添加水印。下图是一个常见的水印嵌入流程。图像经过DCT变换后，把水印数据加到选好的频率系数上，再使用IDCT还原图像，这样水印的嵌入就完成了。DCT嵌入水印流程如果在提取水印时有原始图像作为参考，则图中的嵌入逻辑一般有如下几种选择。式中vi表示原始系数，xi表示水印系数，α为常量。有参时的水印嵌入公式[6]其对应的水印提取流程如下：DCT有参提取水印流程如果没有原始图像做参考，那么可以参考上篇的LSB方法，在嵌入时将原始系数以低精度形式量化，再将水印数据存储在高精度的区域中。基于DWT与SVD的方法上文有提到隐形水印算法需要同时满足低视觉损失和高鲁棒性。能达到这个效果的工具不止DCT，DWT和SVD也是两个常见的选择。其中DWT一般使用Haar小波，其运算量较低，可以将图像分解成不同频带的四份，并且可以递归地执行多次，显著减少后续需要处理的数据量。因此它常被用来做隐形水印的预处理。而SVD则是将图像数据单纯地视为二维矩阵，利用奇异值的稳定性来保护水印。下图是一个结合DWT和SVD的隐形水印例子。图中虽然仅进行了一轮DWT，并选择了LL低频数据进行处理。但实际上也存在使用多轮DWT，以及利用LH和HL数据做SVD的实现。它们对应的水印提取流程也只是个逆过程，这里就不再贴图了。DWT+SVD嵌入水印流程关于变换域方法的实现细节，这里推荐阅读python开源库invisible-watermark。它实现了多种算法，复合使用了DCT、DWT、SVD，代码量也很小，比较适合阅读。另外这个库还实现了一种机器学习水印算法。下文会提到。时间域水印前文提到的几个方法基本都是把视频看作单帧图像来进行处理。对于相比图像多了一个维度（时间维度）的视频来说，当然存在利用这个维度来嵌入水印的例子。关于时间维度的粒度，有的算法是以帧（frame）为单位，有的则是以场景（scene）为单位。帧很好理解，每一帧画面嵌入一定的数据，比如水印的某个比特位，最后组合得到结果。而场景则是指视频的镜头内容，当发生镜头切换时，画面内容会发生大幅度变化，算法认为这时该嵌入下一组数据。它的好处是不受帧率限制，转码后发生帧率变化时依然有效；坏处是场景切换检测的准确度有待商榷。比如笔者看到的论文使用了最简单的SAD（Sum of Absolute Differences）作为切换判断，考虑到现代视频的内容复杂，剪辑手法多样，还存在大量短视频，这种方法很容易出现问题。机器学习水印当我们发现传统图像处理算法效果提升不上去了的时候，基本都可以去试试机器学习。上文提到的python库invisible-watermark中，就提供了一种被称为RivaGAN可选算法来嵌入和提取水印数据。顾名思义，它是一种基于GAN实现的神经网络。在训练过程中，研究者分别使用了一个Critic网络评估画面失真和一个Adversary网络模拟主动攻击，用它们作为对抗源来训练网络，以期同时在画面质量和鲁棒性方面得到较好的结果。另外还有一个人工设计的Noise网络被用来模拟常见的传输失真（包括缩放，裁剪，有损压缩），用以进一步提升模型鲁棒性。下图是RivaGAN的网络结构，github上有它的完整实现。RivaGAN的编解码流程[7]其他方案在搜索过程中，笔者还发现了一些比较少见的方案。比如利用数独的可恢复性，将水印分割成9块，以数独的形式嵌入，从而在画面不完整的情况下通过解数独的方法恢复出水印。摘要里描述这种方法最高可以应对98.8%的画面损失[8]。另外还有一类，是把水印数据嵌入在梯度方向中[9]。这种技术相比其他方法在对抗画面缩放攻击方面有一定优势。水印的混淆与加密使用隐形水印时一般需要公开算法，毕竟当要用水印鉴定所属人时，没有人信任一个黑盒子的提取结果。但公开后，再复杂的水印嵌入方法都有被攻击者提取甚至替换可能性。为了防止这种情况，在嵌入水印时，往往会对水印数据本身或者嵌入的坐标信息进行混淆加密，通过key的形式管理。这样一来，只要攻击者没有密钥，即便他们已经知道水印的嵌入方式，也没办法探测出原有的水印数据。完整的隐形水印系统[10]总结开头也说了，现有的隐形水印算法基本都不够可靠。如果攻击者知道原理，替换水印也许做不到（有混淆的存在），但抹除它还是没问题的。即便不知道原理，只要攻击者能够接受画质损失，那么单单通过屏摄，就能抹除九成九的水印。因为屏摄引入的图像失真是一种复合类型的失真，空域、频域、梯度方向等等都会受到影响，即便是机器学习算法，也不一定能经受得起屏摄的挑战。所以要在生产环境中应用隐形水印的话，一定要清楚它的局限性，适度使用。参考文献[1] CCITT Rec. T.81[2] APPNOTE.TXT - .ZIP File Format Specification[3] Adobe Flash Video File Format Specification[4] Recommendation ITU-T H.264[5] https://en.wikipedia.org/wiki/Frequency_domain [6] I.J. Cox, J. Kilian, F.T. Leighton, T. Shamoon. Secure spread spectrum watermarking for multimedia. 1997.[7] Zhang, Kevin Alex and Xu, Lei and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan. Robust Invisible Video Watermarking with Attention. MIT EECS, September 2019.[8] Mohammad Shahab Goli, Alireza Naghsh. Introducing a New Method Robust Against Crop Attack In Digital Image Watermarking Using Two-Step Sudoku. 2017.[9] Ehsan Nezhadarya, Z. Jane Wang, Rabab Kreidieh Ward. Robust Image Watermarking Based on Multiscale Gradient Direction Quantization. 2011.[10] C.I. Podilchuk, E.J. Delp. Digital watermarking: algorithms and applications. 2001." } ]
